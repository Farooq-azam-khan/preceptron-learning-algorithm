{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preceptron Learning Algorith: A Gentle Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceptron learning algorithm has 2 parts: predicting and traning. Here we will look at both parts. \n",
    "\n",
    "## Predicting with PLA\n",
    "Lets say we have some data formatted like this: `[(x1,y1), (x2,y2), ... (xn,yn)]`. You are given a new point `x` and our job is to predict `y` and we are told that this data fits a linear model. \n",
    "\n",
    "We know the equation of a line is $y=mx+b$ and so we try to figure out a good slope ($m$) and a good y-intercept ($b$). The preceptron algorithm with take the input to the system `x` and give you an output `y`. Our job now is to figure out how the input is manipulated through the blackbox. \n",
    "\n",
    "**PLA** will take the input `x` multiply it by a weight `w`. The weight is initalized to whatever we want to choose or we can ask python to give us a random number. Later we will try to figure out how to set this weight so that it is the best possible. \n",
    "$$ x \\cdot w = y$$\n",
    "We are missing the y-intercept in out model. In machine learning it is refered to as the bias and it can be represented as just anoter weight. Lets represent our input and weights as vectors.  \n",
    "$$\n",
    "x = (x_1, 1) \\\\ \n",
    "w = (w_1, w_2)\n",
    "$$\n",
    "Here $w_1$ is the slope $m$ and $w_2$ is the bias $b$. If we computer the dot product of these two vectors, we will get the equation of our line as above: \n",
    "$$\n",
    "   x \\cdot w = x_1 \\cdot w_1 + 1\\cdot w_2 = y\n",
    "$$\n",
    "\n",
    "The reason we used vectors is so that we can generalized this algorithm. We can have multiple. An example can be seen below: \n",
    "$$\n",
    "x = (x_1, x_2, x_3, 1) \\quad w = (w_1, w_2, w_3, w_4) \n",
    "$$\n",
    "$$\n",
    "y = x \\cdot w \\\\ \n",
    "  = x_1 \\cdot w_1 + x_2  \\cdot w_2 + x_3 \\cdot w_3 + 1 \\cdot w_4\n",
    "$$\n",
    "\n",
    "Note that we always have biases. This all seems abstract; however, this leads to very interesting applications. (TODO give an example). This is called the feed forward algorithm. When we are predicting we are feeding the input forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Feedforward Algorithm\n",
    "The feed forward algorithm works with $n-$dimentional vectors. For the Preceptron it is just the dot product. \n",
    "$$ \\sum_{i=1}^{n} x_i \\cdot w_i = y $$\n",
    "If you were to generalize this idea of a preceptron to multiple preceptrons this idea of a dot product will hold, but there will be multiple dot prodcuts. \n",
    "What we have described above is a regression problem, but often times this algorithm is used to classify between two or more types of outputs. We will define a function `sign` which will return `-1` if the output of the dot product is below zero and `1` if the output is above zero. \n",
    "``` Python \n",
    "sign(val):\n",
    "    if val>= 0:\n",
    "        return -1\n",
    "    return 1\n",
    "```\n",
    "Our output `y` now is: `sign(dot(x, w))`. \n",
    "An implementation of the feed forward algorithm is present in the `src/preceptron/preceptron.py` file. \n",
    "\n",
    "``` Python \n",
    "def feed_forward(inputs, weights):\n",
    "    # add the bias as part of the input\n",
    "    bias = 1\n",
    "    dot_product = sum([xi*wi for xi,wi in zip(inputs+[bias], weights)])\n",
    "    return sign(dot_product)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Weights\n",
    "We must train the weights so that our prediction is as acurate as it can be. We must do this by finding the error in our output, update the weights based on the error and repeat until we get a high accuracy. \n",
    "\n",
    "It is a _ step process: \n",
    "1. find the output with the current weights, lets call it the `guess`; \n",
    "2. you have access to the expected output (lets call it the `target`), figure out the `error = target - guess`; \n",
    "3. update all the weights based on the `error`: `weight[i] = weight[i] + error * x[i] * learning_rate`\n",
    "4. repeat steps 1 to 3 until you get a high accuracy.\n",
    "\n",
    "$$error = target - preditcion \\\\\n",
    "w_i = w_i + error \\cdot x_i \\cdot learning\\_rate$$\n",
    "\n",
    "The above fourmua introduces another concept. The `learning_rate` is a number from 0 to 1 and it is useful because it only updates a percentage of the weight. If the learning rate was not there is a good chace that the weights will bounce around the targets which would result in low accuracy. We want the weihts to update gradually. \n",
    "\n",
    "Below is an example of a trainig function that takes in a sample input and its corresponding target value and updates the weights accordingly.\n",
    "``` Python \n",
    "def train(inputs, weights, target):\n",
    "  # get a guess based on the input (+1 or -1)\n",
    "    guess_input = feed_forward(inputs, weigths)\n",
    "    \n",
    "    # get the error = known answer - guess\n",
    "    error = target - guess_input\n",
    "    \n",
    "    # adjust the weights here (the bias is inclded as one of the weights)\n",
    "    new_weights = []\n",
    "    lr = 0.1\n",
    "    bias = 1\n",
    "    for xi, wi in zip(inputs+[bias], weights):\n",
    "        # change the weights based on the previous weight, the LR,\n",
    "        delta_weight = error * xi * lr\n",
    "        new_weights.append(weight + delta_weight)\n",
    "    return new_weights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: The OR function\n",
    "- The `OR` function is defined to have output `1` if at leat one input is true, otherwise `0`. \n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 0  | 1 | \n",
    "| 1  | 1  | 1 | \n",
    "\n",
    "Therefore out inputs $x = (x_1, x_2)$, our expected outputs are $y=(0,1,1,1)$. Lets trains out PLA on this data and see how it performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vector object\n",
    "import sys\n",
    "sys.path.append('./src/vector/')\n",
    "sys.path.append('./src/preceptron/')\n",
    "from vector import Vector\n",
    "from preceptron import Preceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.49, 0.01, 0.35]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[0,0], [0,1], [1,0], [1,1]]\n",
    "targets = [0,1,1,1]\n",
    "\n",
    "p_or = Preceptron(2) # will generate random weights\n",
    "p_or.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we have random weights, so lets see what the feed forward algorithm predicts if we give it each input data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] => 1\n",
      "[0, 1] => 1\n",
      "[1, 0] => -1\n",
      "[1, 1] => -1\n"
     ]
    }
   ],
   "source": [
    "for xi in data:\n",
    "    print(f'{xi} => {p_or.feed_forward(xi)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are geeting negative number because we design our `sign(x)` function to be as such. We can alter it so that if when we get `1` our output of `y` is `1`, and when we get `-1` from our `feed_forward(x)` algorithm we map the output to be `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] => 1\n",
      "[0, 1] => 1\n",
      "[1, 0] => 0\n",
      "[1, 1] => 0\n"
     ]
    }
   ],
   "source": [
    "def display_output(data):\n",
    "    for xi in data: \n",
    "        ff = p_or.feed_forward(xi)\n",
    "        if ff == -1:\n",
    "            ff = 0\n",
    "        print(f'{xi} => {ff}')\n",
    "display_output(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now notice that the mode is not giving accurate results; thus, we have to train it. Let us train it on all the inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] => 1\n",
      "[0, 1] => 1\n",
      "[1, 0] => 1\n",
      "[1, 1] => 1\n"
     ]
    }
   ],
   "source": [
    "for xi, target in zip(data, targets): \n",
    "    p_or.train(xi, target)\n",
    "display_output(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viola! After training we get the exact function. This was an easy eaxmple, in practice this will almost never happed because there will be alot of noise in the data and you might need to train the model for hundreds and thousands of iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The XOR Function Example\n",
    "Let us try a harder function this time. The `XOR` function is defined to be `1` if the inputs are an odd number of `1`s.\n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 0  | 1 | \n",
    "| 1  | 1  | 0 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] => 0\n",
      "[0, 1] => 0\n",
      "[1, 0] => 0\n",
      "[1, 1] => 0\n"
     ]
    }
   ],
   "source": [
    "# the input data is the same, but the targets are not\n",
    "targets = [0,1,1,0]\n",
    "\n",
    "# some helper functions\n",
    "def train_model(targets):\n",
    "    model = Preceptron(2)\n",
    "    for xi, target in zip(data, targets):\n",
    "        model.train(xi, target)\n",
    "    return model\n",
    "\n",
    "def display_output(data, model):\n",
    "    for xi in data: \n",
    "        ff = model.feed_forward(xi)\n",
    "        if ff == -1:\n",
    "            ff = 0\n",
    "        print(f'{xi} => {ff}')\n",
    "\n",
    "p_xor = train_model(targets)\n",
    "display_output(data, p_xor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this model is function is hard to model with the preceptron. What if we train on all the inputs more than once? Lets try it, maybe in our first try we got unlucky. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] => 1\n",
      "[0, 1] => 1\n",
      "[1, 0] => 1\n",
      "[1, 1] => 1\n"
     ]
    }
   ],
   "source": [
    "def train_model(targets):\n",
    "    model = Preceptron(2)\n",
    "    for _ in range(10):\n",
    "        for xi, target in zip(data, targets):\n",
    "            model.train(xi, target)\n",
    "    return model\n",
    "p_xor = train_model(targets)\n",
    "display_output(data, p_xor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to not work. This is because the xor function is not linearly seperable. Lets try to figure out why. Graph the xor function and try to fit a line so that all the `1`s are on one side and all the `0`s are on the other side of the line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| inputs  | 0 | 1 |\n",
    "|---------|---|---|\n",
    "| 0       | 0 | 1 |\n",
    "| 1       | 1 | 0 | \n",
    "\n",
    "The graph in show in the table above. Try to seperate this data by using only a line. It is impossible. Although the **PLA** cannot solve this problem, the generalized **PLA** (a neural network) can. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
